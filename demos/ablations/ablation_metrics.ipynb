{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce FID and Directional loss\n",
    "Please run the notebook below to reproduce the metrics reported in the blogpost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 13:47:33.913257: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-27 13:47:34.490844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModelWithProjection: ['vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'logit_scale', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'logit_scale', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.weight']\n",
      "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking from: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.052924112514592704, (0.02865201260124684)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.010375436714384704 (0.034696649130475564) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 592.74it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 586.65it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 604.45it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 589.42it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.98it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 571.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 3, gives FID: 63.51802062988281 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 3, gives FID: 83.07181549072266 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 3, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/4/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05424981701653451, (0.029077282131697534)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.011815618958789855 (0.03524624910270311) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 589.88it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 584.07it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 608.06it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 580.71it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 603.50it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 565.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 4, gives FID: 64.53962707519531 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 4, gives FID: 83.50435638427734 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 4, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04319809109903872, (0.028049403709762517)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.0043251440487802025 (0.03422612671652555) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 601.87it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 550.44it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 599.99it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 590.69it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 580.17it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 592.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 0, gives FID: 59.90480422973633 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 0, gives FID: 82.55948638916016 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05135361237451434, (0.029179075568722075)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.008406497654505074 (0.0339523863067974) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.61it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 592.94it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.40it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 587.95it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.63it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 2, gives FID: 63.13673782348633 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 2, gives FID: 82.36078643798828 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 2, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/5/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05403949589002877, (0.029777257854890857)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.011737257905770094 (0.036090003689509195) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 586.08it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 573.41it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 605.97it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 591.01it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 567.07it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 562.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 5, gives FID: 65.4240951538086 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 5, gives FID: 83.48917388916016 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 5, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04718971352092922, (0.025761491950724594)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.0029502938827499748 (0.03429661156932769) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.82it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 578.56it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 606.72it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 585.78it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 605.80it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 588.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 1, gives FID: 63.00893783569336 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 1, gives FID: 89.13209533691406 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.048325642198324206, (0.04546529477756389)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.02357530277222395 (0.05791415518415178) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 545.15it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 544.29it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 553.15it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 518.87it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 582.36it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 555.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 3, gives FID: 90.00294494628906 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 3, gives FID: 150.6565704345703 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 3, gives FID: 148.77557373046875 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/4/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04891325160861015, (0.046796417258550514)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.023185402341187 (0.059933971568093346) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 544.26it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 492.19it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 560.89it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 499.50it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 562.72it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 541.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 4, gives FID: 91.67791748046875 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 4, gives FID: 151.4420928955078 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 4, gives FID: 148.77557373046875 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04879883372224867, (0.03435681550531761)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.01728546399390325 (0.04130440122628239) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 583.86it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 582.81it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 602.38it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 583.40it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 605.55it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 588.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 0, gives FID: 50.60593032836914 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 0, gives FID: 76.72499084472656 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04818859864026308, (0.03746430481366603)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.022598135098814965 (0.04957534211036626) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 549.90it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 512.56it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 512.18it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 543.70it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 575.71it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 539.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 2, gives FID: 90.08039855957031 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 2, gives FID: 147.84141540527344 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 2, gives FID: 148.77557373046875 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/5/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05188205689191818, (0.04611467079291984)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.0197281863540411 (0.05598110140331021) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 568.84it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 492.83it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 524.37it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 544.16it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 575.37it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 559.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 5, gives FID: 92.36187744140625 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 5, gives FID: 150.05322265625 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 5, gives FID: 148.77557373046875 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04959941177628934, (0.03173026996482105)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.016871243128553034 (0.042566203945578335) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 593.08it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 580.94it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.71it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 592.31it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 607.51it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 594.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 1, gives FID: 49.44668960571289 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 1, gives FID: 76.89019012451172 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h1/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.055433748373761776, (0.0326414265613516)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.012402198840864003 (0.0373911555983555) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 600.20it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 585.89it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 607.96it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 590.87it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 608.91it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 574.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h1' on epoch 3, gives FID: 64.80374908447266 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h1' on epoch 3, gives FID: 86.19737243652344 between edited and original\n",
      "ablation: 'heads_ablation/h1' on epoch 3, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h1/4/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05609631897881627, (0.03284964818648055)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.013010276230052114 (0.03727680367324463) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 600.64it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 574.96it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 609.42it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 583.96it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 609.26it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 592.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h1' on epoch 4, gives FID: 65.05773162841797 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h1' on epoch 4, gives FID: 85.30609130859375 between edited and original\n",
      "ablation: 'heads_ablation/h1' on epoch 4, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h1/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04183071969076991, (0.03162620530057887)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.004503773082979024 (0.035104821290535944) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 592.56it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 571.33it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 608.10it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 579.91it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 575.03it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 586.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h1' on epoch 0, gives FID: 60.871238708496094 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h1' on epoch 0, gives FID: 82.6185302734375 between edited and original\n",
      "ablation: 'heads_ablation/h1' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h1/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05480578614864498, (0.031494962737878815)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.011772734480910003 (0.035808949969804674) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.68it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 579.80it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 602.38it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 590.10it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 604.62it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 588.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h1' on epoch 2, gives FID: 63.74927520751953 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h1' on epoch 2, gives FID: 85.90216827392578 between edited and original\n",
      "ablation: 'heads_ablation/h1' on epoch 2, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h1/5/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.056034960420802235, (0.03271935341481282)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.013002871698699892 (0.037406180490064465) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.64it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 577.78it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 606.56it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 577.53it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 610.57it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 571.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h1' on epoch 5, gives FID: 65.35826110839844 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h1' on epoch 5, gives FID: 85.47836303710938 between edited and original\n",
      "ablation: 'heads_ablation/h1' on epoch 5, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h1/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05002437360119075, (0.030780571356429205)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.005732155162841082 (0.03654041494921235) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 582.17it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 577.98it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 605.24it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 584.06it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 605.87it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 575.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h1' on epoch 1, gives FID: 62.02922058105469 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h1' on epoch 1, gives FID: 87.5025634765625 between edited and original\n",
      "ablation: 'heads_ablation/h1' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h8/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04824348082765937, (0.02746230245630335)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.004751621759496629 (0.03432115041060255) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 582.83it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 576.97it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 602.11it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 587.41it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 604.87it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 588.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h8' on epoch 3, gives FID: 62.93404769897461 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h8' on epoch 3, gives FID: 83.32677459716797 between edited and original\n",
      "ablation: 'heads_ablation/h8' on epoch 3, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h8/4/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04934216272085905, (0.029063119408927335)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.00600057709030807 (0.03549052145505208) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 608.31it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.60it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.24it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.99it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.12it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 591.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h8' on epoch 4, gives FID: 63.751522064208984 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h8' on epoch 4, gives FID: 84.115478515625 between edited and original\n",
      "ablation: 'heads_ablation/h8' on epoch 4, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h8/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.045314923040568826, (0.029502280310488467)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.0017711583408527077 (0.03422502637647286) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 608.42it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 593.44it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.66it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.44it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 610.61it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h8' on epoch 0, gives FID: 58.10007858276367 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h8' on epoch 0, gives FID: 87.50086212158203 between edited and original\n",
      "ablation: 'heads_ablation/h8' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h8/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04870466312393546, (0.027628023738772)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.0049786197044886646 (0.03425195488297183) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.19it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 593.89it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.01it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.75it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 592.13it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h8' on epoch 2, gives FID: 62.080055236816406 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h8' on epoch 2, gives FID: 81.75946044921875 between edited and original\n",
      "ablation: 'heads_ablation/h8' on epoch 2, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h8/5/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04845519252121448, (0.029229646906279723)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.005170537801459432 (0.03516800741163937) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 609.74it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 590.22it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.86it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.36it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 612.77it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 599.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h8' on epoch 5, gives FID: 63.61885452270508 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h8' on epoch 5, gives FID: 83.77391815185547 between edited and original\n",
      "ablation: 'heads_ablation/h8' on epoch 5, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h8/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04971065363846719, (0.02865307148143934)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.004255274911411106 (0.035662447510281525) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.69it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.55it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.73it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.23it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 617.27it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 600.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h8' on epoch 1, gives FID: 61.14494323730469 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h8' on epoch 1, gives FID: 83.956787109375 between edited and original\n",
      "ablation: 'heads_ablation/h8' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h4/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04666320856194943, (0.029252338654775743)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.0020969172473996878 (0.03605195520296994) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 589.48it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.44it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.99it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 594.45it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.15it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h4' on epoch 3, gives FID: 61.756099700927734 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h4' on epoch 3, gives FID: 83.52913665771484 between edited and original\n",
      "ablation: 'heads_ablation/h4' on epoch 3, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h4/4/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04765370956622064, (0.028959213403026627)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.0034490154217928647 (0.03552497134318825) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 610.56it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.69it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.24it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.58it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 610.87it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h4' on epoch 4, gives FID: 62.787960052490234 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h4' on epoch 4, gives FID: 83.03034973144531 between edited and original\n",
      "ablation: 'heads_ablation/h4' on epoch 4, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h4/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.042694259495474396, (0.02879829017738296)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.00536347633227706 (0.03428471393028844) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 615.07it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.01it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 610.26it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 593.88it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 616.63it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 600.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h4' on epoch 0, gives FID: 58.64979553222656 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h4' on epoch 0, gives FID: 83.6365737915039 between edited and original\n",
      "ablation: 'heads_ablation/h4' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h4/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.0500969277927652, (0.027857148938493916)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.005781299914233386 (0.034736921061945564) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 610.98it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 574.19it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.37it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.87it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.86it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h4' on epoch 2, gives FID: 61.15873336791992 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h4' on epoch 2, gives FID: 85.62588500976562 between edited and original\n",
      "ablation: 'heads_ablation/h4' on epoch 2, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h4/5/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04781592581886798, (0.028259914004665782)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.003668584614060819 (0.034832011357147655) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 610.77it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.26it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.37it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.35it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 610.98it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h4' on epoch 5, gives FID: 63.714630126953125 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h4' on epoch 5, gives FID: 82.01378631591797 between edited and original\n",
      "ablation: 'heads_ablation/h4' on epoch 5, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h4/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04492357025388628, (0.0267874410811382)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.0009212927101179958 (0.030637873800639436) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.88it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 594.66it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.97it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.36it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.42it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h4' on epoch 1, gives FID: 59.5010871887207 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h4' on epoch 1, gives FID: 85.69596099853516 between edited and original\n",
      "ablation: 'heads_ablation/h4' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "walking from: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/cp_transformer_simple/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.004784872289747, (0.027149416987611006)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.04758397648110986 (0.028516844447961043) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 612.46it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.52it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.68it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.14it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 617.27it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 3, gives FID: 94.35169219970703 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 3, gives FID: 91.3673095703125 between edited and original\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 3, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/cp_transformer_simple/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.009650742961093783, (0.02825398424162976)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.05249505910091102 (0.03312686308325572) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.14it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.15it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.89it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.89it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 620.09it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 593.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 0, gives FID: 105.24352264404297 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 0, gives FID: 90.2609634399414 between edited and original\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/cp_transformer_simple/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.004784872289747, (0.027149416987611006)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.04758397648110986 (0.028516844447961043) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.04it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 592.64it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 612.71it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.28it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 616.30it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 2, gives FID: 94.35169219970703 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 2, gives FID: 91.3673095703125 between edited and original\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 2, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/cp_transformer_simple/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.0023654535319656135, (0.026939365222641495)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.04530824669869617 (0.029312583112615784) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 612.24it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.40it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.51it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.26it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 619.96it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 593.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 1, gives FID: 91.65074157714844 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 1, gives FID: 80.11982727050781 between edited and original\n",
      "ablation: 'layertype_ablation/cp_transformer_simple' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/c_transformer_simple/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.017738692848943173, (0.027668822958799237)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.06152608110569417 (0.028040680881793845) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 612.48it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 593.80it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 612.29it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.12it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.68it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 600.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 3, gives FID: 99.90447998046875 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 3, gives FID: 88.84677124023438 between edited and original\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 3, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/c_transformer_simple/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.012274700668640434, (0.02813643462718602)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.056041637784801424 (0.03296444965260321) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 607.33it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.50it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.57it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.32it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.83it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 593.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 0, gives FID: 113.49461364746094 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 0, gives FID: 100.48361206054688 between edited and original\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/c_transformer_simple/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.017738692848943173, (0.027668822958799237)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.06152608110569417 (0.028040680881793845) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 609.78it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.16it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.04it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.46it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.03it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 2, gives FID: 99.90447998046875 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 2, gives FID: 88.84677124023438 between edited and original\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 2, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/c_transformer_simple/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.015532007701694965, (0.02762029957578292)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.059153529908508064 (0.029862499302327605) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 608.02it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.52it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.83it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 591.03it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 609.46it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 1, gives FID: 104.8729476928711 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 1, gives FID: 90.54297637939453 between edited and original\n",
      "ablation: 'layertype_ablation/c_transformer_simple' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/conv/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.013765578428283333, (0.031264908264803534)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.05655594776384532 (0.03354445066258963) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 615.13it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 594.52it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 612.78it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.37it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 616.90it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/conv' on epoch 3, gives FID: 95.30982971191406 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/conv' on epoch 3, gives FID: 94.68638610839844 between edited and original\n",
      "ablation: 'layertype_ablation/conv' on epoch 3, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/conv/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.01086796136572957, (0.026072015645144764)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.053726082853972915 (0.028127027744738606) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 619.16it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 593.80it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 583.96it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.34it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 623.67it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/conv' on epoch 0, gives FID: 101.96814727783203 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/conv' on epoch 0, gives FID: 90.56658935546875 between edited and original\n",
      "ablation: 'layertype_ablation/conv' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/conv/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.0127515087579377, (0.02742287487387845)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.05522230501752347 (0.03145968146755081) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.79it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 592.01it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 614.35it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.14it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 616.45it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 599.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/conv' on epoch 2, gives FID: 94.8095932006836 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/conv' on epoch 2, gives FID: 92.04373168945312 between edited and original\n",
      "ablation: 'layertype_ablation/conv' on epoch 2, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/conv/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.009401035313494503, (0.02901320695342141)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.05171716107055545 (0.03261226293913635) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.84it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.13it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 615.15it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 599.29it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 617.39it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 600.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/conv' on epoch 1, gives FID: 102.62968444824219 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/conv' on epoch 1, gives FID: 97.63924407958984 between edited and original\n",
      "ablation: 'layertype_ablation/conv' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/pc_transformer_simple/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.007917675538919866, (0.025487539435461858)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.051313180457800624 (0.02773179382703193) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 619.41it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.76it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.83it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.45it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 622.07it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 600.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 3, gives FID: 98.31036376953125 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 3, gives FID: 100.76753234863281 between edited and original\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 3, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/pc_transformer_simple/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.01131975302938372, (0.025579645525270468)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.0540021342644468 (0.029247009420625074) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 616.65it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.64it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 612.66it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 592.55it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 622.68it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 0, gives FID: 105.47683715820312 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 0, gives FID: 113.15132141113281 between edited and original\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/pc_transformer_simple/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.010201000303495675, (0.027655164140256348)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.05343779338058084 (0.03143021371349022) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.78it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 594.14it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.67it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 584.00it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 619.02it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 599.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 2, gives FID: 101.35345458984375 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 2, gives FID: 104.93273162841797 between edited and original\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 2, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/pc_transformer_simple/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.00802123305387795, (0.025731706329800652)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.05080765868537128 (0.028659093096704714) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 616.84it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.19it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.69it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.83it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 615.43it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 599.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 1, gives FID: 93.16931915283203 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 1, gives FID: 94.18939208984375 between edited and original\n",
      "ablation: 'layertype_ablation/pc_transformer_simple' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/p_transformer_simple/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.011198547976091504, (0.027105878506585822)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.05455230691470206 (0.030932388993328737) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 630.53it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.89it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 615.48it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 599.58it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 637.13it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 3, gives FID: 108.02643585205078 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 3, gives FID: 112.72565460205078 between edited and original\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 3, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/p_transformer_simple/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.011335461824201047, (0.02488913421853494)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.0538884930498898 (0.02991330275178364) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 609.22it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 594.27it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 612.62it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.45it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 618.11it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 0, gives FID: 98.7154312133789 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 0, gives FID: 89.50467681884766 between edited and original\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/p_transformer_simple/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.011198547976091504, (0.027105878506585822)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.05455230691470206 (0.030932388993328737) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 634.29it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.89it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 612.24it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.97it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 637.14it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 601.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 2, gives FID: 108.02643585205078 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 2, gives FID: 112.72565460205078 between edited and original\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 2, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/p_transformer_simple/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: -0.004163689836859703, (0.023810866173065243)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.04611961293965578 (0.027472759424438005) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 606.13it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 595.17it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 613.96it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.03it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 611.50it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 599.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 1, gives FID: 95.32453918457031 between edited and reconstructed\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 1, gives FID: 89.91558074951172 between edited and original\n",
      "ablation: 'layertype_ablation/p_transformer_simple' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append(\"../../src/lib/\")\n",
    "sys.path.append(\"../../src/lib/asyrp\")\n",
    "\n",
    "from lib_utils.metrics import DirectionalSimilarity, calculate_fid\n",
    "from transformers import (CLIPImageProcessor, CLIPTextModelWithProjection,\n",
    "                          CLIPTokenizer, CLIPVisionModelWithProjection)\n",
    "from utils.text_dic import SRC_TRG_TXT_DIC\n",
    "\n",
    "# see text in text_dic.py\n",
    "GUID = \"pixar\"\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "(src_texts, target_texts) = SRC_TRG_TXT_DIC[GUID]\n",
    "\n",
    "head_abl_path = Path(\"/home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/\")\n",
    "layer_abl_path = Path(\"/home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/\")\n",
    "basepaths = [head_abl_path,layer_abl_path]\n",
    "results = {}\n",
    "\n",
    "clip_id = \"openai/clip-vit-large-patch14\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(clip_id)\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(clip_id).to(DEVICE)\n",
    "image_processor = CLIPImageProcessor.from_pretrained(clip_id)\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(clip_id).to(DEVICE)\n",
    "\n",
    "dir_similarity = DirectionalSimilarity(tokenizer, text_encoder, image_processor, image_encoder)\n",
    "\n",
    "for basepath in basepaths:\n",
    "    print(f'walking from: {basepath}')\n",
    "    for path in os.walk(basepath, topdown=False):\n",
    "\n",
    "        # find ablation paths\n",
    "        if \"40/edited\" in str(path):\n",
    "            print(f\"parsing: {path[0]}\")\n",
    "            path = path[0] + \"/\"\n",
    "            # path magic\n",
    "            epoch = path.split(\"/40\")[0][-1]\n",
    "            ablation_type = path.split(\"eval_runs/\")[-1]\n",
    "            ablation_type = ablation_type.split(\"/40\")[0]\n",
    "            ablation_type, epoch = ablation_type[:-2], ablation_type[-1:]\n",
    "            \n",
    "            path_edited = path\n",
    "            path_recon = path.replace(\"edited\", \"reconstructed\")\n",
    "            path_original = path.replace(\"edited\", \"original\")\n",
    "\n",
    "            list_original_path = glob(path_original + \"/*.png\")\n",
    "\n",
    "            scores_or = []\n",
    "            scores_oe = []\n",
    "            scores_re = []\n",
    "\n",
    "            for i in range(len(list_original_path)):\n",
    "                img_original_path = str(Path(path_original) / f\"test_{i}_19_ngen40_original.png\")\n",
    "                img_reconstructed_path = str(Path(path_recon) / f\"test_{i}_19_ngen40_reconstructed.png\")\n",
    "                img_edited_path = str(Path(path_edited) / f\"test_{i}_19_ngen40_edited.png\")\n",
    "                \n",
    "                original_image = Image.open(img_original_path)\n",
    "                reconstructed_image = Image.open(img_reconstructed_path)\n",
    "                edited_image = Image.open(img_edited_path)\n",
    "                \n",
    "                similarity_score_or = dir_similarity(original_image, reconstructed_image, src_texts, src_texts)\n",
    "                similarity_score_oe = dir_similarity(original_image, edited_image, src_texts, target_texts)\n",
    "                similarity_score_re = dir_similarity(reconstructed_image, edited_image, src_texts, target_texts)\n",
    "\n",
    "                scores_or.append(float(similarity_score_or.detach().cpu()))\n",
    "                scores_oe.append(float(similarity_score_oe.detach().cpu()))\n",
    "                scores_re.append(float(similarity_score_re.detach().cpu()))\n",
    "\n",
    "            sdir_or = np.mean(scores_or)\n",
    "            sdir_or_var = np.std(scores_or)\n",
    "            sdir_oe = np.mean(scores_oe)\n",
    "            sdir_oe_var = np.std(scores_oe)\n",
    "            sdir_re = np.mean(scores_re)\n",
    "            sdir_re_var = np.std(scores_re)\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"Attribute {GUID} gives original-reconstructed CLIP directional similarity: {sdir_or} ({sdir_or_var})\")\n",
    "            print(f\"Attribute {GUID} gives original-edited CLIP directional similarity: {sdir_oe}, ({sdir_oe_var})\")\n",
    "            print(f\"Attribute {GUID} gives reconstructed-edited CLIP directional similarity: {sdir_re} ({sdir_re_var}) \")\n",
    "\n",
    "            score_er = calculate_fid(path_edited, path_recon)\n",
    "            score_ro = calculate_fid(path_original, path_recon)\n",
    "            score_eo = calculate_fid(path_edited, path_original)\n",
    "            print(\"calculating FID\")\n",
    "\n",
    "            print(f\"ablation: '{ablation_type}' on epoch {epoch}, gives FID: {score_er} between edited and reconstructed\")\n",
    "            print(f\"ablation: '{ablation_type}' on epoch {epoch}, gives FID: {score_eo} between edited and original\")\n",
    "            print(f\"ablation: '{ablation_type}' on epoch {epoch}, gives FID: {score_ro} between reconstructed and original\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            run_results = {\n",
    "                \"sdir_or\": float(sdir_or),\n",
    "                \"sdir_or_var\": float(sdir_or_var),\n",
    "                \"sdir_oe\": float(sdir_oe),\n",
    "                \"sdir_oe_var\": float(sdir_oe_var),\n",
    "                \"sdir_re\": float(sdir_re),\n",
    "                \"sdir_re_var\": float(sdir_re_var),\n",
    "                \"fid_edited_reconstructed\": score_er,\n",
    "                \"fid_edited_original\": score_eo,\n",
    "                \"fid_reconstructed_original\": score_ro,\n",
    "                \"epochs\": epoch,\n",
    "                \"ablation_name\": ablation_type,\n",
    "                \"attr\": GUID\n",
    "            }\n",
    "            \n",
    "            results[f\"{ablation_type}_{epoch}_{GUID}\"] = run_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to json file\n",
    "with open(\"metrics.json\", \"w\") as f:\n",
    "    f.write(json.dumps(results, indent=4))\n",
    "    results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asyrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
