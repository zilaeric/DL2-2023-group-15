{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce FID and Directional loss\n",
    "Please run the notebook below to reproduce the metrics reported in the blogpost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 13:05:33.244927: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-27 13:05:33.834989: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModelWithProjection: ['vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPVisionModelWithProjection: ['text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'logit_scale', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking from: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.052924112514592704, (0.02865201260124684)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.010375436714384704 (0.034696649130475564) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 545.83it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 460.99it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 484.01it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 529.92it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 596.47it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 551.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 3, gives FID: 63.51802062988281 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 3, gives FID: 83.07181549072266 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 3, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/4/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05424981701653451, (0.029077282131697534)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.011815618958789855 (0.03524624910270311) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 562.75it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 554.18it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 566.78it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 574.58it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 585.98it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 568.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 4, gives FID: 64.53962707519531 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 4, gives FID: 83.50435638427734 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 4, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04319809109903872, (0.028049403709762517)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.0043251440487802025 (0.03422612671652555) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 569.53it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 565.44it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 587.93it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 572.32it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 588.77it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 556.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 0, gives FID: 59.90480422973633 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 0, gives FID: 82.55948638916016 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05135361237451434, (0.029179075568722075)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.008406497654505074 (0.0339523863067974) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 557.49it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 540.32it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 569.48it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 527.87it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 580.20it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 568.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 2, gives FID: 63.13673782348633 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 2, gives FID: 82.36078643798828 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 2, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/5/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05403949589002877, (0.029777257854890857)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.011737257905770094 (0.036090003689509195) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 570.38it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 544.51it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 578.28it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 569.82it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 573.43it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 569.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 5, gives FID: 65.4240951538086 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 5, gives FID: 83.48917388916016 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 5, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h2/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04718971352092922, (0.025761491950724594)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.0029502938827499748 (0.03429661156932769) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 559.39it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 540.60it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 574.76it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 557.35it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 520.70it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 565.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h2' on epoch 1, gives FID: 63.00893783569336 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h2' on epoch 1, gives FID: 89.13209533691406 between edited and original\n",
      "ablation: 'heads_ablation/h2' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.048325642198324206, (0.04546529477756389)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.02357530277222395 (0.05791415518415178) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 567.86it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 443.48it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 535.18it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 524.20it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 569.06it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 547.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 3, gives FID: 90.00294494628906 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 3, gives FID: 150.6565704345703 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 3, gives FID: 148.77557373046875 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/4/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04891325160861015, (0.046796417258550514)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.023185402341187 (0.059933971568093346) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 517.62it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 457.51it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 490.00it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 458.87it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 572.21it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 520.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 4, gives FID: 91.67791748046875 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 4, gives FID: 151.4420928955078 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 4, gives FID: 148.77557373046875 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04879883372224867, (0.03435681550531761)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.01728546399390325 (0.04130440122628239) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 586.85it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 565.34it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 587.20it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 580.69it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 593.95it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 558.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 0, gives FID: 50.60593032836914 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 0, gives FID: 76.72499084472656 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04818859864026308, (0.03746430481366603)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.022598135098814965 (0.04957534211036626) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 555.82it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 420.27it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 530.28it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 439.28it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 562.83it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 477.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 2, gives FID: 90.08039855957031 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 2, gives FID: 147.84141540527344 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 2, gives FID: 148.77557373046875 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/5/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05188205689191818, (0.04611467079291984)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.0197281863540411 (0.05598110140331021) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 439.09it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 508.86it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 556.05it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 519.92it/s]\n",
      "parsing src images for fid: 100%|██████████| 5/5 [00:00<00:00, 554.27it/s]\n",
      "parsing target images for fid: 100%|██████████| 5/5 [00:00<00:00, 517.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 5, gives FID: 92.36187744140625 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 5, gives FID: 150.05322265625 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 5, gives FID: 148.77557373046875 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/conv/1/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04959941177628934, (0.03173026996482105)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.016871243128553034 (0.042566203945578335) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 590.38it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 538.17it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 576.29it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 559.69it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 591.31it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 560.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/conv' on epoch 1, gives FID: 49.44668960571289 between edited and reconstructed\n",
      "ablation: 'heads_ablation/conv' on epoch 1, gives FID: 76.89019012451172 between edited and original\n",
      "ablation: 'heads_ablation/conv' on epoch 1, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h1/3/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.055433748373761776, (0.0326414265613516)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.012402198840864003 (0.0373911555983555) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 573.91it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 565.68it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 590.61it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 561.73it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 598.20it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 564.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h1' on epoch 3, gives FID: 64.80374908447266 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h1' on epoch 3, gives FID: 86.19737243652344 between edited and original\n",
      "ablation: 'heads_ablation/h1' on epoch 3, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h1/4/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05609631897881627, (0.03284964818648055)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.013010276230052114 (0.03727680367324463) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 585.21it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 554.14it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 563.43it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 569.20it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 593.92it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 562.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h1' on epoch 4, gives FID: 65.05773162841797 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h1' on epoch 4, gives FID: 85.30609130859375 between edited and original\n",
      "ablation: 'heads_ablation/h1' on epoch 4, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h1/0/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.04183071969076991, (0.03162620530057887)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: -0.004503773082979024 (0.035104821290535944) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 586.34it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 564.32it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 561.61it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 599.51it/s]\n",
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 608.16it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 603.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating FID\n",
      "ablation: 'heads_ablation/h1' on epoch 0, gives FID: 60.871238708496094 between edited and reconstructed\n",
      "ablation: 'heads_ablation/h1' on epoch 0, gives FID: 82.6185302734375 between edited and original\n",
      "ablation: 'heads_ablation/h1' on epoch 0, gives FID: 80.81753540039062 between reconstructed and original\n",
      "--------------------------------------------------\n",
      "parsing: /home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/h1/2/40/edited\n",
      "==================================================\n",
      "Attribute pixar gives original-reconstructed CLIP directional similarity: 0.0 (0.0)\n",
      "Attribute pixar gives original-edited CLIP directional similarity: 0.05480578614864498, (0.031494962737878815)\n",
      "Attribute pixar gives reconstructed-edited CLIP directional similarity: 0.011772734480910003 (0.035808949969804674) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing src images for fid: 100%|██████████| 100/100 [00:00<00:00, 616.95it/s]\n",
      "parsing target images for fid: 100%|██████████| 100/100 [00:00<00:00, 597.00it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "sys.path.append(\"../src/lib/\")\n",
    "sys.path.append(\"../src/lib/asyrp\")\n",
    "\n",
    "from transformers import (\n",
    "    CLIPTokenizer,\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    CLIPImageProcessor,\n",
    ")\n",
    "\n",
    "from lib_utils.metrics import DirectionalSimilarity, calculate_fid\n",
    "from utils.text_dic import SRC_TRG_TXT_DIC\n",
    "\n",
    "# see text in text_dic.py\n",
    "GUID = \"pixar\"\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "(src_texts, target_texts) = SRC_TRG_TXT_DIC[GUID]\n",
    "\n",
    "head_abl_path = Path(\"/home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/heads_ablation/\")\n",
    "layer_abl_path = Path(\"/home/parting/master_AI/DL2/DL2-2023-group-15/src/eval_runs/layertype_ablation/\")\n",
    "basepaths = [head_abl_path,layer_abl_path]\n",
    "results = {}\n",
    "\n",
    "clip_id = \"openai/clip-vit-large-patch14\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(clip_id)\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(clip_id).to(DEVICE)\n",
    "image_processor = CLIPImageProcessor.from_pretrained(clip_id)\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(clip_id).to(DEVICE)\n",
    "\n",
    "dir_similarity = DirectionalSimilarity(tokenizer, text_encoder, image_processor, image_encoder)\n",
    "\n",
    "for basepath in basepaths:\n",
    "    print(f'walking from: {basepath}')\n",
    "    for path in os.walk(basepath, topdown=False):\n",
    "\n",
    "        # find ablation paths\n",
    "        if \"40/edited\" in str(path):\n",
    "            print(f\"parsing: {path[0]}\")\n",
    "            path = path[0] + \"/\"\n",
    "            # path magic\n",
    "            epoch = path.split(\"/40\")[0][-1]\n",
    "            ablation_type = path.split(\"eval_runs/\")[-1]\n",
    "            ablation_type = ablation_type.split(\"/40\")[0]\n",
    "            ablation_type, epoch = ablation_type[:-2], ablation_type[-1:]\n",
    "            \n",
    "            path_edited = path\n",
    "            path_recon = path.replace(\"edited\", \"reconstructed\")\n",
    "            path_original = path.replace(\"edited\", \"original\")\n",
    "\n",
    "            list_original_path = glob(path_original + \"/*.png\")\n",
    "\n",
    "            scores_or = []\n",
    "            scores_oe = []\n",
    "            scores_re = []\n",
    "\n",
    "            for i in range(len(list_original_path)):\n",
    "                img_original_path = str(Path(path_original) / f\"test_{i}_19_ngen40_original.png\")\n",
    "                img_reconstructed_path = str(Path(path_recon) / f\"test_{i}_19_ngen40_reconstructed.png\")\n",
    "                img_edited_path = str(Path(path_edited) / f\"test_{i}_19_ngen40_edited.png\")\n",
    "                \n",
    "                original_image = Image.open(img_original_path)\n",
    "                reconstructed_image = Image.open(img_reconstructed_path)\n",
    "                edited_image = Image.open(img_edited_path)\n",
    "                \n",
    "                similarity_score_or = dir_similarity(original_image, reconstructed_image, src_texts, src_texts)\n",
    "                similarity_score_oe = dir_similarity(original_image, edited_image, src_texts, target_texts)\n",
    "                similarity_score_re = dir_similarity(reconstructed_image, edited_image, src_texts, target_texts)\n",
    "\n",
    "                scores_or.append(float(similarity_score_or.detach().cpu()))\n",
    "                scores_oe.append(float(similarity_score_oe.detach().cpu()))\n",
    "                scores_re.append(float(similarity_score_re.detach().cpu()))\n",
    "\n",
    "            sdir_or = np.mean(scores_or)\n",
    "            sdir_or_var = np.std(scores_or)\n",
    "            sdir_oe = np.mean(scores_oe)\n",
    "            sdir_oe_var = np.std(scores_oe)\n",
    "            sdir_re = np.mean(scores_re)\n",
    "            sdir_re_var = np.std(scores_re)\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"Attribute {GUID} gives original-reconstructed CLIP directional similarity: {sdir_or} ({sdir_or_var})\")\n",
    "            print(f\"Attribute {GUID} gives original-edited CLIP directional similarity: {sdir_oe}, ({sdir_oe_var})\")\n",
    "            print(f\"Attribute {GUID} gives reconstructed-edited CLIP directional similarity: {sdir_re} ({sdir_re_var}) \")\n",
    "\n",
    "            score_er = calculate_fid(path_edited, path_recon)\n",
    "            score_ro = calculate_fid(path_original, path_recon)\n",
    "            score_eo = calculate_fid(path_edited, path_original)\n",
    "            print(\"calculating FID\")\n",
    "\n",
    "            print(f\"ablation: '{ablation_type}' on epoch {epoch}, gives FID: {score_er} between edited and reconstructed\")\n",
    "            print(f\"ablation: '{ablation_type}' on epoch {epoch}, gives FID: {score_eo} between edited and original\")\n",
    "            print(f\"ablation: '{ablation_type}' on epoch {epoch}, gives FID: {score_ro} between reconstructed and original\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            run_results = {\n",
    "                \"sdir_or\": float(sdir_or),\n",
    "                \"sdir_or_var\": float(sdir_or_var),\n",
    "                \"sdir_oe\": float(sdir_oe),\n",
    "                \"sdir_oe_var\": float(sdir_oe_var),\n",
    "                \"sdir_re\": float(sdir_re),\n",
    "                \"sdir_re_var\": float(sdir_re_var),\n",
    "                \"fid_edited_reconstructed\": score_er,\n",
    "                \"fid_edited_original\": score_eo,\n",
    "                \"fid_reconstructed_original\": score_ro,\n",
    "                \"epochs\": epoch,\n",
    "                \"ablation_name\": ablation_type,\n",
    "                \"attr\": GUID\n",
    "            }\n",
    "            \n",
    "            results[f\"{ablation_type}_{epoch}_{GUID}\"] = run_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to json file\n",
    "with open(\"metrics.json\", \"w\") as f:\n",
    "    f.write(json.dumps(results, indent=4))\n",
    "    results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asyrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
